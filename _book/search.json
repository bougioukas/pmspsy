[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Statistical Methods for Analyzing Empirical Data",
    "section": "",
    "text": "Welcome!\nThis course introduces postgraduate students to advanced statistical methods used in the quantitative social, educational, and behavioral sciences. Students will learn:\n\nData management.\nAnalysis of Variance (ANOVA) Designs. Moderator analysis.\nMultiple Linear Regression. Interaction between variables.\nFactor Analysis.\nMediation and Path Analysis.\nBasic Structural Equation Modeling (SEM).\nPrinciples of Evidence Synthesis (Systematic reviews, Meta-analysis, Umbrella reviews).\n\n \nThe students will also learn about the principles of data visualization and statistical software JAMOVI.\n     \nSources for reading\n\nAnalysis of Variance Designs: A Conceptual and Computational Approach with SPSS and SAS\nLearning Statistics with Jamovi",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction to Jamovi and data preparation",
    "section": "",
    "text": "1.1 Why Jamovi?\nJamovi is a new fee open “3rd generation” statistical software that is built on top of the programming language R (Figure 1.1). Designed from the ground up to be easy to use, Jamovi is a compelling alternative to costly statistical products such as SPSS and SAS.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Jamovi and data preparation</span>"
    ]
  },
  {
    "objectID": "introduction.html#why-jamovi",
    "href": "introduction.html#why-jamovi",
    "title": "1  Introduction to Jamovi and data preparation",
    "section": "",
    "text": "Figure 1.1: Jamovi is free and open statistical software\n\n\n\n\n\n\n\n\n\nSome other advantages are:\n\n\n\n\nUser-friendly point-and-click interface.\nDisplays informative tables and clear visuals.\nSupports add-on modules for advanced statistical analysis.\nAllows integration with R.\nProvides access to a user guide and community resources on the Jamovi website.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Jamovi and data preparation</span>"
    ]
  },
  {
    "objectID": "introduction.html#downloading-and-installing-jamovi",
    "href": "introduction.html#downloading-and-installing-jamovi",
    "title": "1  Introduction to Jamovi and data preparation",
    "section": "1.2 Downloading and installing Jamovi",
    "text": "1.2 Downloading and installing Jamovi\nJamovi is available for Windows (64-bit), macOS, Linux and ChromeOS. Installation on desktop is quite straight-forward. Just go to the Jamovi download page https://www.jamovi.org/download.html, and download the latest version (current release) for your operating system.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Jamovi and data preparation</span>"
    ]
  },
  {
    "objectID": "introduction.html#navigating-jamovi",
    "href": "introduction.html#navigating-jamovi",
    "title": "1  Introduction to Jamovi and data preparation",
    "section": "1.3 Navigating Jamovi",
    "text": "1.3 Navigating Jamovi\nWhen jamovi first opens, we will see a screen something like in Figure 1.2.\n\n\n\n\n\n\n\n\nFigure 1.2: Jamovi starts up!.\n\n\n\n\n\nTo the left is the spreadsheet view, and to the right is where the results of statistical tests appear. Down the middle is a bar separating these two regions, and this can be dragged to the left or the right to change their sizes.\n \nLet’s take a quick look at the Jamovi Main Menu, referred to hereafter as the Menu, as shown in Figure 1.3. This Menu is displayed at the very top of the Jamovi screen:\n\n\n\n\n\n\n\n\nFigure 1.3: The menu bar provides access to all functions of the program.\n\n\n\n\n\nThere are six tabs in the Menu (from left to right): 1. File (a layer with three horizontal levels \\(\\equiv\\)), 2. Variables, 3. Data, 4. Analyses, 5. Edit, and 6. Settings (the three dots \\(\\vdots\\) at the top right of the window) tabs. A toolbar appears whenever we click on a Menu tab (Table 1.1).\n \n\n\n\nTable 1.1: Menu and toolbars of Jamovi\n\n\n\n\n\n\n\n\n\nMenu tab\nToolbar\n\n\n\n\n\nFile tab (\\(\\equiv\\))\nThe file tab  allows us to open/import existing files, save and export our files.\n\n\n\n\n\nVariables tab\nThis allows us to view and search our variables in a list view.\n\n\nThis view allows us to easily navigate our variables and do the following:\n\nSearch for a variable by scrolling through the list or search for one by name.\nEdit the variable names and descriptions by double-clicking in the relevant field.\nEdit our variable details by double-clicking on the data symbol (the screen will appear for us to add all the necessary information).\nCreate a new variable by clicking on the in the bottom right corner. |\n\n\n\n\nData tab\nHere we will see our raw data which are organised like Excel in rows and columns. We can also manipulate our data and add new variables when necessary.\n\n\nSpecifically, this tab allows us to do the following:\n\nRename and add details to existing variables. Click on the Setup button, or double-click on the variable we want to manage.\nCompute and transform variables\nAdd and/or Delete variables (columns)\nAdd Filters\nAdd and/or Delete Rows\n\n\n\n\nAnalyses tab\nIt includes the available statistical analyses that can be performed by Jamovi.\n\n\nWe will spend most of our time in the Analyses Tab. The following six modules are pre-installed:\n\nExploration\nT-Tests\nANOVA\nRegression\nFrequencies\nFactor\n\nFor example, if we want to perform regression analysis, we simply click the ’’Regression” button.\nAll other modules need to be installed using the Modules button (Plus button) in our top-right  |\n\n\n\nEdit tab\nIt includes a toolbar similar to a word processor.\n\n\nWe can add extra information to our results using the buttons that are very similar to what we would find in Word (though there are fewer options).\n\n\n\nSettings tab\n(the three dots \\({\\vdots}\\) at the top right of the window)\n\nIt includes the application settings that can be manged by the users according to their preferences.\n\n\nWe can apply our preferences for a number of settings such as:\n\nHow many decimal numbers we want.\nIf we want to learn R, we can also display the R syntax.\nOur graph color scheme.\nOur default missing value.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Jamovi and data preparation</span>"
    ]
  },
  {
    "objectID": "introduction.html#types-of-variables-in-jamovi",
    "href": "introduction.html#types-of-variables-in-jamovi",
    "title": "1  Introduction to Jamovi and data preparation",
    "section": "1.4 Types of Variables in Jamovi",
    "text": "1.4 Types of Variables in Jamovi\nData variables can be one of four measure types:\n\n Nominal: This type is for nominal categorical variables.\n Ordinal: This type is for ordinal categorical variables.\n Continuous : this type is for variables with numeric values which are considered to be of Interval or Ratio scales.\n ID: This will usally be our first column. This can be text or numbers, but it should be unique to each row.\n\n \nAdditionally, data variables can be one of three data types:\n\nInteger: These are full numbers e.g. 1, 2, 3, … 100, etc. - Integers can be used for all three measure types . When used for Nominal/Ordinal data numbers will represent labels e.g. male=1; female=2.\nDecimal: These are numbers with decimal points. e.g. 1.3, 5.6, 7.8, etc. - This will usually only be used for continuous data.\nText: This can be used for ordinal and nominal data.\n\nThe measure types are designated by the symbol in the header of the variable’s column. Note that some combinations of data-type and measure-type don’t make sense, and Jamovi won’t let us choose these.\n\n\n\nTable 1.2: Types of data and measures\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure\n\n\n\n\n\nData\nNominal\nOrdinal\nContinuous\n\n\nInteger\n\\({\\checkmark}\\)\n\\({\\checkmark}\\)\n\\({\\checkmark}\\)\n\n\nDecimal\n\n\n\\({\\checkmark}\\)\n\n\nText\n\\({\\checkmark}\\)\n\\({\\checkmark}\\)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Jamovi and data preparation</span>"
    ]
  },
  {
    "objectID": "introduction.html#importing-data",
    "href": "introduction.html#importing-data",
    "title": "1  Introduction to Jamovi and data preparation",
    "section": "1.5 Importing data",
    "text": "1.5 Importing data\n\n1.5.1 The dataset\nIt is possible to simply begin typing values into the Jamovi spreadsheet as we would with any other spreadsheet software. Alternatively, existing datasets in a range of formats (OMV, Excel, CSV, SPSS, R data, Stata, SAS) can be opened in Jamovi. We will use the following dataset as an example (Figure 1.4).\n\n\n\n\n\n\n\n\nFigure 1.4: Table with raw data.\n\n\n\n\n(NOTE: You can find other formats of the data (OMV, Excel) at the link: https://osf.io/gvctz/).\n   \nThe meta-data (data about the data) for this dataset are as following:\n\nsex: sex (1 = male, 2 = female).\nage: age in years.\ntime_spend: hours spent on social media.\n\\(q_1 ... q_{10}\\): Ten questions (items) of Rosenberg Self-Esteem Scale (RSES). The 10 items are answered on a four point scale ranging from strongly agree to strongly disagree coded as follows: Strongly Agree = 3, Agree = 2, Disagree = 1, and Strongly Disagree = 0.\n\n\nPositively worded Items 1, 2, 4, 6, and 7.\nNegatively worded Items 3, 5, 8, 9, and 10 (codes should be reversed).\n\n\n\n\n\n\n\nFigure 1.5: Codes should be reversed for the negatively worded Items.\n\n\n\nThe scale ranges from 0-30 (we add the scores for all items), with 30 indicating the highest total score possible.\nMore information for the RSES: Rosenberg Self-Esteem Scale.\n\n\n1.5.2 Opening the file\nTo open this csv file, click on the File tab  at the top left hand corner (just left of the Variables tab) (Figure 1.6).\n\n\n\n\n\n\nFigure 1.6: Click on the File tab\n\n\n\nThis will open the menu shown in Figure 1.7. Select ‘Open’ and then ‘This PC’. Choose the downloaded file from the files listed on ‘Browse’ which are stored on our computer folders:\n\n\n\n\n\n\n\n\nFigure 1.7: Open an existing file stored on our computer into Jamovi.\n\n\n\n\n\n \nThe flowchart of the process is:\n\n\n\n\n\nflowchart LR\n  A(File tab) -.-&gt; B(Open) -.-&gt; C(This PC) -.-&gt; D(Browse) -.-&gt; E(Open \\n 'the downloaded file')\n\n\n\n\n\n\n \nWe should see data now in the Spreadsheet view (Figure 1.8).\n\n\n\n\n\n\n\n\nFigure 1.8: Our dataset.\n\n\n\n\n\nAs we can see this is a data set with 258 observations and 13 variables. JAMOVI has classified all the variables as nominal ; however, only the sex variable is actually nominal.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Jamovi and data preparation</span>"
    ]
  },
  {
    "objectID": "introduction.html#adding-labels-to-codes",
    "href": "introduction.html#adding-labels-to-codes",
    "title": "1  Introduction to Jamovi and data preparation",
    "section": "1.6 Adding labels to codes",
    "text": "1.6 Adding labels to codes\n\nsex variable\n\nThe first variable, named sex, is a categorical variable coded as 1 for males and 2 for females. Notice that it is correctly identified as a nominal  variable in Jamovi.\nWe can assign labels to numerically coded values of categorical variables, such as sex, by accessing the data variable settings. One way to achieve this is by double-clicking on the variable name sex, which opens the additional menu of variable settings at the top of the Jamovi screen (Figure 1.9).\n\n\n\n\n\n\nFigure 1.9: An additional menu appears at the top of the Jamovi screen after double-clicking on the variable name sex.\n\n\n\n \nIn this menu, we will find the Levels setup. Here, we can specify the labels that should appear for each category level. Click on the number “1” in the Levels box to edit its label, changing it from “1” to “male”. Similarly, click on the number “2” and change it to “female” (Figure 1.10).\n\n\n\n\n\n\nFigure 1.10: Adding labels to numerically coded Values.\n\n\n\nNotice how the numbers “1” and “2” have moved to the lower right under the text we’re typing, allowing us to still see which label corresponds to each numerical code. Press Enter or click anywhere outside the labels box to save these labels.\nWe close the variable settings by pressing the arrow in the top-right corner .",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Jamovi and data preparation</span>"
    ]
  },
  {
    "objectID": "introduction.html#changing-the-measure-type",
    "href": "introduction.html#changing-the-measure-type",
    "title": "1  Introduction to Jamovi and data preparation",
    "section": "1.7 Changing the measure type",
    "text": "1.7 Changing the measure type\nNext, we will change the measure type of age and time_spent variables from nominal to continuous.\n\nage variable\n\nDouble-click on the variable name age to open the data variable settings, as shown in Figure 1.11:\n\n\n\n\n\n\nFigure 1.11: Data variable menu settings for the age variable.\n\n\n\n \nFrom the drop-down list of “Measure type” we select the continuous type , as shown in Figure 1.12.\n\n\n\n\n\n\nFigure 1.12: Change the measure type of age from nominal to continuous. .\n\n\n\n \n\ntime_spent variable\n\nInstead of closing the data variable menu using the arrow in the top-right corner, we can click on the  to proceed to the next variable setting, time_spent. As before, we select the continuous type for this variable from the “Measure type” drop-down list, as shown in Figure 1.13.\n\n\n\n\n\n\nFigure 1.13: Change the measure type of time_spent from nominal to continuous.\n\n\n\nWe close the variable settings by pressing the arrow in the top-right corner .\n \n\nq1 to q10 variables\n\nFinally, we will change the measure type of q1 to q10 variables from nominal to ordinal. In the Variables tab, we select the checkboxes for the q1 through q10 variables, as shown in Figure 1.14.\n\n\n\n\n\n\nFigure 1.14: Change the measure type of time_spent from nominal to continuous.\n\n\n\n \nAfter that, we click on the Edit button  to open the data variable settings, as shown in Figure 1.15.\n\n\n\n\n\n\nFigure 1.15: Change the measure type of time_spent from nominal to continuous.\n\n\n\n \nFrom the drop-down list of “Measure type” we select the ordinal type , as shown in Figure 1.16.\n\n\n\n\n\n\nFigure 1.16: Change the measure type of q1 to q10 from nominal to ordinal.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Jamovi and data preparation</span>"
    ]
  },
  {
    "objectID": "introduction.html#filtering-rows",
    "href": "introduction.html#filtering-rows",
    "title": "1  Introduction to Jamovi and data preparation",
    "section": "1.8 Filtering rows",
    "text": "1.8 Filtering rows\nNext, we select the Filters button  from the Data tab. This opens the “Row FILTERS” view at the top of the Jamovi screen where we can add a filter called “Filter 1” (Figure 1.17). Let’s say that we want to study only the adults from the participants in this survey.\n\nSimple condition\n\nIn order to access functions, press the  icon in the filter settings and from “VARIABLES” double-click on age (or we just type the variable name but if the variable has a space, we must use ticks '' around the variable name). Then type the condition age &gt;= 18 in the formula box and press ENTER from the keyboard (Figure 1.17).\n\n\n\n\n\n\nFigure 1.17: Adding a filter.\n\n\n\n \n\n\n\n\n\n\nRelational (or comparison) operators in Jamovi\n\n\n\n\n\n\nsymbol\nread as\n\n\n\n\n&lt;\nless than\n\n\n&gt;\ngreater than\n\n\n==\nequal to\n\n\n&lt;=\nless than or equal to\n\n\n&gt;=\ngreater than or equal to\n\n\n!=\nnot equal to\n\n\n\n\n\n \nNotice that a column named Filter 1 has been added to the Spreadsheet view. Cells that meet the condition age &gt;= 18 are checked with a green tick , while the rest have a red x symbol . Lines with an X are grayed out indicating that these observations are now outside of the current dataset.\nThere is also a switch where we can activate  or inactivate  the filter (note that an inactivate filter will remain visible and can be toggled to active at any time).\nIt is also possible to hide all filter columns by clicking on the eye symbol  of the filters. In this case, all filters and the filtered data will remain active but will be invisible.\nFinally, if we want to delete the filter permanently, we can click on the  of the filter .\n \n\nMultiple conditions\n\nBut we can do more complicated filters than this! Let’s say that we’re interested in the adult females. In fact we can specify this in three ways:\na) by using the and operator in “Filter 1” which means that both conditions (adults and females) in the expression must be true at the same time. Therefore, we type the expression age &gt;= 18 and sex == 'female' (Figure 1.18):\n\n\n\n\n\n\nFigure 1.18: Combining conditions with “and” operator in one expression.\n\n\n\nNote that in the above expression we can also use double quotes around the \"female\" (i.e., age &gt;= 18 and sex == \"female\") or even the coded value (i.e., age &gt;= 18 and sex == 2).\nb) by adding the second condition (i.e. sex == \"female\") as another expression to “Filter 1” (by clicking the small + beside the first expression) (Figure 1.19):\n\n\n\n\n\n\nFigure 1.19: Multiple expressions in the same filter.\n\n\n\nThis additional expression comes to be represented with its own column F1(2) (Figure 1.19), and by looking at the ticks and crosses, we can see which expression is responsible for excluding each row.\nc) adding a new “Filter 2” (by selecting the large + to the left of the filters dialog box) (Figure 1.20). In this case, we can activate or inactivate the filters separately.\n\n\n\n\n\n\nFigure 1.20: Multiple expressions using multiple filters.\n\n\n\nWe close the filter settings by pressing the arrow in the top-right corner .\n\n\n\n\n\n\nImportant\n\n\n\nFilters in Jamovi exclude the rows for which the expression is not true. When filters are active, all results will be based on the filtered data. If we want to see unfiltered results we will either need to delete the filter or toggle it to inactive.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Jamovi and data preparation</span>"
    ]
  },
  {
    "objectID": "introduction.html#transforming-a-variable",
    "href": "introduction.html#transforming-a-variable",
    "title": "1  Introduction to Jamovi and data preparation",
    "section": "1.9 Transforming a variable",
    "text": "1.9 Transforming a variable\n\n1.9.1 Transform a quantitative variable into a qualitative variable\nSometimes it can be useful to convert a quantitative variable into a qualitative variable with levels. For example, the time_spent can be categorized as follows:\n\nless than or equal to 3  “0-3”\n4 to 7  “4-7”\n8 to 11  “8-11”\ngreater than 11  “&gt;11”\n\nSelect time_spent variable and click the Transform button  from the toolbar of the Data tab. This opens the “TRANSFORMED VARIABLE” view, where we can set the name of the transformed variable such as time_spent2 and create the transformation. To do this, select the source variable, the time_spent in this case, in the Source Variable field and create a new transformation using transform field (Figure 1.21), which is initially set to ‘none’.\n\n\n\n\n\n\nFigure 1.21: Setting up a new transformation of a variable.\n\n\n\n \nThis opens the “TRANSFORM” view where Jamovi gives each transformation a name (e.g., Transform 1; if we want we can change it). This allows us to use it again later on other variables if we wish. We can also add a description (Figure 1.22).\n\n\n\n\n\n\nFigure 1.22: Box for adding conditions to the transformation.\n\n\n\nNow we need to add conditions. Jamovi uses simple if ... else statements and executes each statement starting from the top. So let’s start!\nFirst, select + Add recode condition. Second, we need to fill the boxes with the information as follows (Figure 1.23):\n\nThe $source is the variable we want to transform (here time_spent)- don’t change this.\nSelect the appropriate comparison operator (here &lt;= )\nIn the next box, we will put the time_spent value we want as the cut off point (e.g., 3).\nAfter the use, add our new label (here '0-3'). If we are using text we must enclose it in '...'\n\n\n\n\n\n\n\nFigure 1.23: Adding the first condition (0-3).\n\n\n\n \nWe can add as many conditions as we want by selecting + Add recode condition. This will add a new if $source line into the box (Figure 1.24). Remember they will be executed in order.\n\n\n\n\n\n\nFigure 1.24: Adding the second condition (4-7).\n\n\n\n \n\n\n\n\n\n\nFigure 1.25: Adding the third condition (8-11).\n\n\n\n \nFinally, after the else use box just add the label for the data that does not meet the above conditions (here '&gt;11'), as shown in Figure 1.26.\n\n\n\n\n\n\nFigure 1.26: Adding the final label for the data that does not meet the above conditions (&gt;11).\n\n\n\n \n\n\n1.9.2 Reverse scoring of items\nTo compute the total score for all items of the RSES, we first need to reverse the scores of the negatively worded questions (3, 5, 8, 9, and 10). To do this, we can follow these simple steps.\nIn the Variables tab, we select the checkboxes for the q3, q5, q8, q9, and q10 variables, as shown in Figure 1.27.\n\n\n\n\n\n\nFigure 1.27: Select the variables to be reversed.\n\n\n\n \nAfter that, we click on the Transform button ) from the toolbar of the Variables tab to open the Transform variable settings. Then, we Create New Transform, as shown in Figure 1.28.\n\n\n\n\n\n\nFigure 1.28: Open the Transform variable settings.\n\n\n\n \nIn the Transform view, we enter _R as a suffix for the variable names and specify 3-$source in the condition box, as shown in Figure 1.29.\n\n\n\n\n\n\nFigure 1.29: Transform variable settings.\n\n\n\n\n\n\n\n\n\nFigure 1.30: Data with the reversed variables.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Jamovi and data preparation</span>"
    ]
  },
  {
    "objectID": "introduction.html#compute-a-new-variable",
    "href": "introduction.html#compute-a-new-variable",
    "title": "1  Introduction to Jamovi and data preparation",
    "section": "1.10 Compute a new variable",
    "text": "1.10 Compute a new variable\nFinally, we compute each participant’s total score (ranging from 0 to 30) based on their responses to the 10 questions of the Rosenberg Self-Esteem Scale.\nAdding computed variables to a Jamovi spreadsheet is straightforward. Click on the Compute button ) from the toolbar of the Data tab. An empty column has been created at the end of our dataset (Figure 1.31). The black dot symbol in the right of the column header indicates that this is a computed variable.\n\n\n\n\n\n\nFigure 1.31: Compute a new variable.\n\n\n\nTo set up the computed variable, either double-click the column header, or click the Setup button ) in the Data tab. This opens the “COMPUTED VARIABLE” view, where we can name the computed variable score. Next, we select the SUM function from the available options for use in the Formula box. Finally, we add the q-variables in the formula separated by comma (note that we must use the reversed variables in the sum) and we press ENTER (Figure 1.32).\n\n\n\n\n\n\nFigure 1.32: Computation of the total score.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Jamovi and data preparation</span>"
    ]
  },
  {
    "objectID": "anova1.html",
    "href": "anova1.html",
    "title": "2  ANOVA Designs (1)",
    "section": "",
    "text": "2.1 One-way between-subjects Analysis of Variance (One-way ANOVA)\nThe one-way between-subjects analysis of variance (one-way ANOVA) is used to compare more than two independent (unrelated or unpaired) samples. We may think of it as an extension of Student’s t-test.\nAlthough, ANOVA can detect whether there are mean differences between groups, it does not identify which groups are different from the others. At first, we might consider to compare all groups in pairs with t-tests. However, this approach can lead to incorrect conclusions due to the multiple comparisons problem. Each additional t-test increases the likelihood of making at least one Type I error (false positive) across the set (often called a family) of comparisons.\nThis is why, after an ANOVA test concludes that at least one difference exists between groups (omnibus analysis), we should perform statistical tests that account for the number of comparisons (post hoc tests). Some of the most commonly used post hoc tests include the Tukey test, Bonferroni correction, and Holm test.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ANOVA Designs (1)</span>"
    ]
  },
  {
    "objectID": "anova1.html#one-way-between-subjects-analysis-of-variance-one-way-anova",
    "href": "anova1.html#one-way-between-subjects-analysis-of-variance-one-way-anova",
    "title": "2  ANOVA Designs (1)",
    "section": "",
    "text": "2.1.1 Importing data\nThe SAT is used by a wide range of colleges and universities as part of the application process for college admission. Assume we are interested in the effect of preparation time on SAT performance.\nIn this example, we have five student groups, each containing seven cases. The groups contain students who have studied for either zero, two, four, six, or eight months prior to taking the SAT.\n\n\n\n\n\n\n\n\nFigure 2.1: Table with raw data.\n\n\n\n\nOpen the dataset named “sat” from the file tab in the menu (Figure 2.2).\n\n\n\n\n\n\nFigure 2.2: The sat dataset.\n\n\n\n \nWe prepare the data as follows (Figure 2.3):\n\n\n\n\n\n\nFigure 2.3: The modified dataset.\n\n\n\n\n\n2.1.2 Research question\nA quaziexperimental study explored the effect of preparation time on SAT performance, with students having studied for different duration (zero, two, four, six, or eight months). The question of interest is whether the mean SAT score differs across these preparation times.\n\n\n2.1.3 Hypothesis testing for the one-way ANOVA test\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\\(H_0\\): all group means are equal (the means of SAT in the five conditions are equal: \\(\\mu_{1} = \\mu_{2} = \\mu_{3} = \\mu_{4} = \\mu_{5}\\))\n\\(H_1\\): at least one group mean differs from the others (there is at least one group with mean SAT score different from the others)\n\n\n\n\n\n2.1.4 Assumptions\n\nThe dependent variable, satscore, should be approximately normally distributed for all groups. To increase the number of observations used in assessing normality, we often examine the residuals from the ANOVA model for the entire dataset using a normal quantile plot of the standardized residuals (Normal Q-Q plot).\nThe data in groups have similar variance (also named as homogeneity of variance or homoscedasticity).\n\n\n\n2.1.5 Descriptive statistics and plots\nOn the Jamovi top menu navigate to\n\n\n\n\n\nflowchart LR\n  A(Exploration) -.-&gt; B(Descriptives)\n\n\n\n\n\n\nas shown below in Figure 2.4.\n\n\n\n\n\n\nFigure 2.4: Select exploration and Desctriptives.\n\n\n\nHighlight satscore in the left panel and click it (or drug it) over the the Variables. Then highlight the group and click it over the Split by (Figure 2.5). Additionally, select variables across rows.\n\n\n\n\n\n\nFigure 2.5: Descriptive dialog box.\n\n\n\n \nThe default descriptive statistics are shown in Figure 2.6: the mean, median, standard deviation, minimum and maximum.\n\n\n\n\n\n\nFigure 2.6: Descriptive statistics.\n\n\n\nTo generate boxplots, click the Plots, and check Box plot, Data, and Mean.\n\n\n\n\n\n\nFigure 2.7: Selection of boxplots.\n\n\n\n\n\n\n\n\n\nFigure 2.8: Boxplots.\n\n\n\nThe plot suggests that the mean SAT score (black square) increases as the months progress, suggesting a positive association between time and SAT score improvement.\n\n\n2.1.6 ANOVA (omnibus analysis)\nOn the Jamovi top menu navigate to\n\n\n\n\n\nflowchart LR\n  A(Analyses) -.-&gt; B(ANOVA) -.-&gt; C(ANOVA)\n\n\n\n\n\n\nas shown below in Figure 4.4.\n\n\n\n\n\n\nFigure 2.9: Conducting ANOVA test in Jamovi. In the menu at the top, choose Analyses -&gt; ANOVA  -&gt; ANOVA.\n\n\n\nNOTE: There is also a One-Way ANOVA menu option. This version of the ANOVA analysis does not have all the options we want so we are not going to use this method, so we will run our analysis via “ANOVA” rather than “One-Way ANOVA”.\n \nIn the ANOVA dialog box, highlight satscore in the left panel and click it (or drug it) over the the Dependent Variable. Then highlight the group and click it over the Fixed Factors (Figure 2.10). Additionally, check \\(\\eta^2\\).\n\n\n\n\n\n\nFigure 2.10: ANOVA dialog box.\n\n\n\n \nAssumptions Checks\nClick the Assumptions Checks, and check Homogeneity test, Normality test, and Q-Q plot (Figure 4.9).\n\n\n\n\n\n\nFigure 2.11: Assumption selections for ANOVA.\n\n\n\n\nNormality of distributions\n\n\n\n\n\n\n\nRemember: Hypothesis testing for Shapiro-Wilk test for normality\n\n\n\n\\(H_{0}\\): the data came from a normally distributed population.\n\\(H_{1}\\): the data tested are not normally distributed.\n\nIf p − value &lt; 0.05, reject the null hypothesis, \\(H_{0}\\).\nIf p − value ≥ 0.05, do not reject the null hypothesis, \\(H_{0}\\).\n\n\n\n\n\n\n\n\n\nFigure 2.12: Normality test.\n\n\n\nThe Shapiro-Wilk test of normality suggests normal distributions (p=0.56 &gt; 0.05; \\(H_o\\) is not rejected).\n \n\n\n\n\n\n\nFigure 2.13: Normal Q-Q plot.\n\n\n\nThe data points mostly fall along the diagonal line, indicating that the residuals are approximately normally distributed. Additionally, there are no extreme deviations or systematic patterns, suggesting that normality holds well.\n \n\nEquality of variances\n\n\n\n\n\n\n\nRemember: Hypothesis testing for Levene’s test for equality of variances\n\n\n\n\\(H_{0}\\): the variances of WeightLoss in all groups are equal (\\(σ^2_1=σ^2_2=σ^2_3=σ^2_4=σ^2_5\\))\n\\(H_{1}\\): the variances of satscore differ between groups (\\(σ^2_i\\neq σ^2_j\\), where \\(i,j= 1, 2, 3, 4, 5\\) and \\(i\\neq j\\))\n\nIf p − value &lt; 0.05, reject the null hypothesis, \\(H_{0}\\).\nIf p − value ≥ 0.05, do not reject the null hypothesis, \\(H_{0}\\).\n\n\n\n\n\n\n\n\n\nFigure 2.14: Levene’s test.\n\n\n\nSince p = 0.239 &gt; 0.05, the \\(H_0\\) of the Levene’s test is not rejected and the variances of the five conditions are comparable; in short, it appears that the assumption of homogeneity of variance is not violated.\n \nANOVA table\n\n\n\n\n\n\nFigure 2.15: Summary table of ANOVA.\n\n\n\n \nIn Figure 2.15, the F-statistic is calculated as follows:\n\\[F= \\frac{Mean \\ Square \\ between \\ groups}{Mean \\ Square \\ within \\ groups} = \\frac{Mean \\ Square \\ group}{Mean \\ Square \\ residuals} = \\frac{57624.3}{1325.7} = 43.467\\]\nNote that we compare this value to an F-distribution (F-test). The degrees of freedom in the numerator (df1) and the denominator (df2) are 4 and 30, respectively.\nThe p-value &lt; 0.001 (reject \\(H_0\\) of the ANOVA test). There is at least one condition with mean SAT score which is different from the other means.\nThis table also presents the eta squared (\\(\\eta^2 = 0.85\\)) , which expresses the proportion of variability explained by the group relative to the total variability:\n\\[\\eta^2 = \\frac{Sum \\ of \\ squares \\ group}{Total \\ sum \\ of \\ squares} = \\frac{230497}{230497 + 39771} = \\frac{230497}{270268}=0.85\\]\nA value of 0.85 (85%) means that 85% of the variation in SAT scores can be attributed to the preparation time and would be considered a large effect size. (NOTE: Whether the \\(\\eta^2\\) value is considered “high” or not is relative and depends on the research context).\n \n\n\n2.1.7 Post hoc tests\nClick the Post Hoc Tests, then highlight the group in the left panel and click it (or drug it) over the the right panel. Check Tukey correction.\n\n\n\n\n\n\nFigure 2.16: Post Hoc Tests panels.\n\n\n\n \n\n\n\n\n\n\nFigure 2.17: Post Hoc Tests (Tukey correction).\n\n\n\nFor example, the mean difference between zero months and two months is: 412.857 - 474.286 = -61.429, which is significant (p=0.028).\n \n\nInterpretation\nAn analysis of variance showed that the amount of preparation for the SAT in which students engaged appeared to significantly affect their performances on the test, F(4, 30) = 43.47, p &lt; 0.001, \\(\\eta^2 = 0.85\\). Post-hoc analyses with Tukey’s test, adjusting p-values for multiple comparisons, indicated that each additional two months of study up to six months was associated with significantly higher SAT scores. However, there was no significant difference in scores between the six month and eight month study groups.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>ANOVA Designs (1)</span>"
    ]
  },
  {
    "objectID": "anova2.html",
    "href": "anova2.html",
    "title": "3  ANOVA Designs (2)",
    "section": "",
    "text": "3.1 One-way within-subjects Analysis of Variance (repeated one-way ANOVA)\nThe one-way repeated measures analysis of variance (also known as a within-subjects ANOVA) is an extension of the paired t-test designed to assess whether there are significant differences in the means of three or more related groups, such as comparing the difference between three or more time points.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ANOVA Designs (2)</span>"
    ]
  },
  {
    "objectID": "anova2.html#one-way-within-subjects-analysis-of-variance-repeated-one-way-anova",
    "href": "anova2.html#one-way-within-subjects-analysis-of-variance-repeated-one-way-anova",
    "title": "3  ANOVA Designs (2)",
    "section": "",
    "text": "3.1.1 Importing data\nIn hypothetical study, it is hoped that a certain drug will alleviate the intensity of symptoms of a certain disease. Symptom intensity was evaluated on a twelve-point scale with higher values indicating more intense symptoms. Two pre-treatment baseline measurements are made, the first a month prior to treatment and the second one week prior to treatment. Post-treatments measures of symptom intensity are made after one week, one month, and one year following administration of the drug. In this simplified example, we have eight patients in the study.\n\n\n\n\n\n\n\n\nFigure 3.1: Table with raw data.\n\n\n\n\nOpen the dataset named “symptoms” from the file tab in the menu (Figure 3.2).\n\n\n\n\n\n\nFigure 3.2: The symptoms dataset.\n\n\n\n \nWe prepare the data as follows (Figure 3.3):\n\n\n\n\n\n\nFigure 3.3: The modified dataset.\n\n\n\n\n\n3.1.2 Research question\nA study investigated the effect of a drug on symptom intensity in a specific disease, assessing eight patients before (two pre-test baseline measures) and after treatment (three post-test measures). The primary question is whether the mean symptom intensity score changed over time.\n\n\n3.1.3 Hypothesis testsing for the one-way repeated ANOVA test\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\\(H_0\\): all related group means are equal (the means of symptom intensity score in the five time points are equal: \\(\\mu_{1} = \\mu_{2} = \\mu_{3} = \\mu_{4} = \\mu_{5}\\))\n\\(H_1\\): at least one group mean differs from the others (there is at least one group with mean symptom intensity score different from the others)\n\n\n\n\n\n3.1.4 Assumptions\n\nThe data are normally distributed in all time points.\nThe variances of the differences between all possible pairs of within-subject conditions are equal (sphericity assumption).\n\n\n\n3.1.5 ANOVA (omnibus analysis)\nOn the Jamovi top menu navigate to\n\n\n\n\n\nflowchart LR\n  A(Analyses) -.-&gt; B(ANOVA) -.-&gt; C(Repeated Measures ANOVA)\n\n\n\n\n\n\nas shown below in Figure 3.4.\n\n\n\n\n\n\nFigure 3.4: Conducting ANOVA test in Jamovi. In the menu at the top, choose Analyses -&gt; ANOVA  -&gt; Repeated Measures ANOVA.\n\n\n\n \nIn the repeated ANOVA dialog box, define the levels of the “RM Factor 1” which we will rename as time. Then, highlight the variables from pre1 to post3 in the left panel and click them (or drug them) to the Repeated Measure Cells. Additionally, check \\(\\eta^2\\) and Partial \\(\\eta^2\\) (Figure 3.5).\n\n\n\n\n\n\nFigure 3.5: Repeated ANOVA dialog box.\n\n\n\n \n\n\n3.1.6 Descriptive statistics and plots\n\n\n\n\n\n\nFigure 3.6: The estimated marginal means panels.\n\n\n\n\n\n\n\n\n\nFigure 3.7: Descriptive statistics.\n\n\n\n\n\n\n\n\n\nFigure 3.8: Plot with means.\n\n\n\nThe graph shows a clear reduction in mean symptom intensity after the treatment.\n \nAssumptions Checks\nClick on Assumptions Checks, select Sphericity tests, and Q-Q Plot (Figure 3.9).\n\n\n\n\n\n\nFigure 3.9: Assumption selections for repeated ANOVA.\n\n\n\n\nNormality of distributions\n\n\n\n\n\n\n\nFigure 3.10: Normal Q-Q plot.\n\n\n\n\nSphericity assumption\n\nThis assumption is usually checked with the Mauchly’s sphericity test, where null hypothesis states that the variances of the differences are equal (Figure 3.11).\n\n\n\n\n\n\nFigure 3.11: Assumption selections for repeated ANOVA.\n\n\n\nIn our example, the assumption of sphericity has not been met (p = 0.043). In this case, we have to correct the degrees of freedom in repeated ANOVA analysis.\nThere are two correction options \\(\\epsilon\\) available: Greenhouse-Geisser (GGe), or Huynh-Feldt (HFe) (Figure 3.12). The general recommendation is to use the Greenhouse-Geisser \\(\\epsilon\\) correction when it is less than 0.75; otherwise, we should use the Huynh-Feldt \\(\\epsilon\\) correction.\nAs the GGe value is less than 0.75, we use the Greenhouse-Geisser adjustment of 0.488.\n\n\n\n\n\n\nFigure 3.12: Assumption selections for repeated ANOVA.\n\n\n\nThe corrected degrees of freedom are:\n\\(df_1*GGe=4*0.488=1.95\\)\nand\n\\(df_2*HFe=28*0.488=13.66\\).\n \nRepeated ANOVA table\n\n\n\n\n\n\nFigure 3.13: Summary table of repeated ANOVA.\n\n\n\nAs we can see from Figure 3.13, the within-subjects variable (time) was statistically significant under the Greenhouse-Geisser correction, F(1.95, 13.66) = 18.624, p &lt; 0.001, \\(\\eta^2_p = 0.727\\).\n\n\n3.1.7 Post hoc tests\nClick the Post Hoc Tests, then highlight the time in the left panel and click it (or drug it) over the the right panel. Check Bonferroni correction.\n\n\n\n\n\n\nFigure 3.14: Post Hoc Tests panels.\n\n\n\n \n\n\n\n\n\n\nFigure 3.15: Post Hoc Tests (Bonferroni correction).\n\n\n\n \n\nInterpretation\nBased on Greenhouse-Geisser correction for violation of sphericity, a one-way within-subjects ANOVA revealed a significant difference in the pre-test and post-test means, F(1.95, 13.66) = 18.62, p &lt; 0.001, within-subjects \\(\\eta^2 = 0.73\\). Pairwise comparisons using a Bonferroni correction to maintain an alpha level of 0.05 revealed that intensity of symptoms remained constant from the two pre-test baseline measures, significantly decreased after a week following drug therapy, and further significantly decreased after one month. Symptom intensity did not significantly differ from that level at the end of a year.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ANOVA Designs (2)</span>"
    ]
  },
  {
    "objectID": "anova3.html",
    "href": "anova3.html",
    "title": "4  ANOVA Designs (3)",
    "section": "",
    "text": "4.1 Two-way between-subjects Analysis of Variance (Two-way ANOVA)\nA research design is not limited to examining the effects of a single independent variable. In this section, we will explore how to incorporate a second independent variable. A design that includes multiple independent variables is known as a factorial design, where each level of one independent variable is combined with each level of the other.\nAfter obtaining a significant ANOVA result, it is essential to conduct post hoc tests that account for the number of comparisons to ensure accurate statistical interpretation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>ANOVA Designs (3)</span>"
    ]
  },
  {
    "objectID": "anova3.html#two-way-between-subjects-analysis-of-variance-two-way-anova",
    "href": "anova3.html#two-way-between-subjects-analysis-of-variance-two-way-anova",
    "title": "4  ANOVA Designs (3)",
    "section": "",
    "text": "4.1.1 Importing data\nAssume that researchers are interested in exploring the role of analogical thinking in the problem-solving skills of children and adolescents. The researchers sample 21 students from primary school (6th grade students from “Dimotiko”, ages 11-12) and 21 students from secondary school (3rd grade students from “Gymnasium”, ages 14-15). The students are then randomly assigned to one of three groups, each consisting of 7 students: a control group, experimental group 1 (exposure to similar examples without instructions), and experimental group 2 (exposure to similar examples with instructions). The outcome measured is the number of mistakes made while attempting to solve the problems.\n\n\n\n\n\n\n\n\nFigure 4.1: Table with raw data.\n\n\n\n\nOpen the dataset named “mistakes” from the file tab in the menu (Figure 4.2).\n\n\n\n\n\n\nFigure 4.2: The mistakes dataset.\n\n\n\n \nWe prepare the data as follows (Figure 4.3):\n\n\n\n\n\n\nFigure 4.3: The modified dataset.\n\n\n\n\n\n4.1.2 Research question\nThe question of interest is whether the effect of school age and instructions on the number of mistakes. Specifically, we want to answer the following questions:\n\nDoes the number of mistakes differ based on instructions?\nDoes it differ based on school age?\nDoes the effect of instructions on the number of mistakes depend on school age (moderator)?\n\n\n\n4.1.3 Hypothesis testing for the one-way ANOVA test\n\n\n\n\n\n\nNull and alternative hypotheses\n\n\n\nMain effect: intervention\n\n\\(H_0\\): There is no significant difference in number of mistakes between intervention groups (\\(\\mu_{control} = \\mu_{exper1} = \\mu_{exper2}\\))\n\\(H_1\\): At least one intervention has a significantly different mean number of mistakes.\n\nMain effect: school age\n\n\\(H_0\\): There is no significant difference in number of mistakes between primary and secondary school. (\\(\\mu_{primary} = \\mu_{secondary}\\))\n\\(H_1\\): There is a significant difference in number of mistakes between primary and secondary school (\\(\\mu_{primary} \\neq \\mu_{secondary}\\)).\n\nInteraction effect between intervention and school age\n\n\\(H_0\\): There is no interaction effect between intervention and school age on number of mistakes (i.e., the effect of intervention on number of mistakes does not depend on school age).\n\\(H_1\\): There is a significant interaction effect between intervention and school age on number of mistakes (i.e., the effect of intervention on number of mistakes varies depending on school age).\n\n\n\n\n\n4.1.4 Assumptions\n\nNormality of Residuals: The number of mistakes should be approximately normally distributed within each group (i.e., for each combination of gender and residential community).\nHomogeneity of Variance (Homoscedasticity): The variance of number of mistakes should be approximately equal across all groups. (also named as homogeneity of variance or homoscedasticity).\n\n\n\n4.1.5 ANOVA (omnibus analysis)\nOn the Jamovi top menu navigate to\n\n\n\n\n\nflowchart LR\n  A(Analyses) -.-&gt; B(ANOVA) -.-&gt; C(ANOVA)\n\n\n\n\n\n\nas shown below in Figure 4.4.\n\n\n\n\n\n\nFigure 4.4: Conducting ANOVA test in Jamovi. In the menu at the top, choose Analyses -&gt; ANOVA  -&gt; ANOVA.\n\n\n\n \nIn the ANOVA dialog box, highlight intervention in the left panel and drug it to the the Dependent Variable. Then highlight the gender and school_age and drug them to the Fixed Factors (Figure 4.5). Additionally, check \\(\\eta^2\\).\n\n\n\n\n\n\nFigure 4.5: ANOVA dialog box.\n\n\n\n \n\n\n4.1.6 Descriptive statistics and plots\n\n\n\n\n\n\nFigure 4.6: The estimated marginal means panels.\n\n\n\n\n\n\n\n\n\nFigure 4.7: Descriptive statistics.\n\n\n\n\n\n\n\n\n\nFigure 4.8: Plot with means.\n\n\n\nWe have created an interaction plot that illustrates the simple effects of intervention for primary and secondary school students. The resulting plot shows an interaction because the lines are not parallel. In this example, school age is a moderator.\n \nAssumptions Checks\nClick the Assumptions Checks, and check Homogeneity test, Normality test, and Q-Q plot (Figure 4.9).\n\n\n\n\n\n\nFigure 4.9: Assumption selections for ANOVA.\n\n\n\n\n\n\n\n\n\nFigure 4.10: Normality test.\n\n\n\nThe Shapiro-Wilk test of normality suggests normal distributions (p=0.08 &gt; 0.05; \\(H_o\\) is not rejected).\n \n\n\n\n\n\n\nFigure 4.11: Normal Q-Q plot.\n\n\n\nThe data points mostly fall along the diagonal line, indicating that the residuals are approximately normally distributed.\n \n\nEquality of variances\n\n\n\n\n\n\n\nFigure 4.12: Levene’s test.\n\n\n\nSince p = 0.19 &gt; 0.05, the \\(H_0\\) of the Levene’s test is not rejected and the variances are comparable.\n \nANOVA table\n\n\n\n\n\n\nFigure 4.13: Summary table of ANOVA.\n\n\n\nWe observe that both main effects—intervention (F = 14.2, p &lt;0.001) and school age (F = 48.9, p &lt;0.001), are significant. Additionally, the interaction is also significant (F = 23.1, p &lt;0.001).\nA key principle in interpreting and reporting factorial analysis results is that interactions take precedence over main effects. This is because interactions offer a more detailed and comprehensive understanding of the data.\nTherefore, we can conduct a simple effects analysis (which can be performed in the Linear Models module) and follow up with post hoc tests.\n\n\n\n\n\n\nFigure 4.14: Simple effects (school age as moderator).\n\n\n\nThe number of mistakes in the intervention groups differs significantly only among secondary school students (F = 36.7, p &lt;0.001).\n \n\n\n\n\n\n\nFigure 4.15: Simple effects (intervention as moderator).\n\n\n\nThe number of mistakes between primary school and secondary school students are significant for the experimental groups (F = 12.2, p = 0.001 and F = 82.8, p &lt;0.001).\n\n\n4.1.7 Post hoc tests\nClick the Post Hoc Tests, then highlight the gender x residence in the left panel and click it over the the right panel. Check Tukey correction.\n\n\n\n\n\n\nFigure 4.16: Post Hoc Tests panels.\n\n\n\n \n\n\n\n\n\n\nFigure 4.17: Post Hoc Tests (Bonferroni correction).\n\n\n\n \n\nInterpretation\nSimple effects tests were conducted using the Tukey adjustment to maintain an alpha level of 0.05. Results showed that the effect of instructions on the number of mistakes depends on school age.\nPrimary school students showed a relatively stable number of mistakes across all three groups, with their mistakes remaining high even with experimental interventions.\nSecondary school students demonstrated a significant decrease in mistakes. Specifically, there was a significant reduction in mistakes when comparing the control group to the group exposed to examples (MD = 12.6, p = 0.019). Adding instructions led to an even greater reduction in mistakes (MD = 18.9, p &lt;0.001), indicating that explicit guidance further enhanced their problem-solving skills.\nAdditionally, secondary school students made significantly fewer mistakes than primary school students in both experimental interventions, suggesting that older children may have a stronger ability to apply analogical reasoning in problem-solving after being exposed to similar examples and instructions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>ANOVA Designs (3)</span>"
    ]
  },
  {
    "objectID": "linear1.html",
    "href": "linear1.html",
    "title": "5  Linear regression (1)",
    "section": "",
    "text": "5.1 Introduction to simple linear regression\nSimple linear regression involves a numeric dependent (or response) variable \\(Y\\) and one independent (or explanatory) variable \\(X\\) that is either numeric or categorical.\nOften it is of interest to quantify the linear association between two numeric variables, \\(X\\) and \\(Y\\), and given the value of one variable for an individual, to predict the value of the other variable. This is not possible from the correlation coefficient as it simply indicates the strength of the association as a single number; in order to describe the association between the values of the two variables, a technique called regression is used. In regression, we assume that a change in the independent variable, \\(X\\), will lead directly to a change in the dependent variable \\(Y\\). However, the term “dependent” does not necessarily imply a cause-and-effect relationship between the two variables.\nWe may recall from secondary/high school algebra that the equation of a line is: \\[y = \\beta_o + \\beta_1 \\cdot x \\tag{5.1}\\]\nThe Equation 5.1 is defined by two coefficients (parameters) \\(\\beta_o\\) and \\(\\beta_1\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear regression (1)</span>"
    ]
  },
  {
    "objectID": "linear1.html#introduction-to-simple-linear-regression",
    "href": "linear1.html#introduction-to-simple-linear-regression",
    "title": "5  Linear regression (1)",
    "section": "",
    "text": "Figure 5.1: The equation of line.\n\n\n\n\n\nThe intercept coefficient \\(\\beta_o\\) is the value of \\(y\\) when \\(x = 0\\) (the point where the fitted line crosses the y-axis; Figure 5.1).\nThe slope coefficient \\(\\beta_1\\) for \\(x\\) is the mean change in \\(y\\) for every one unit increase in \\(x\\) (Figure 5.1).\n\n\n5.1.1 Importing data\n\n\n\n\n\n\n\n\nFigure 5.2: Table with raw data.\n\n\n\n\nOpen the dataset named “BirthWeight” from the file tab in the menu (Figure 5.3).\n\n\n\n\n\n\nFigure 5.3: The BirthWeight dataset.\n\n\n\nData of 550 infants at 1 month age was collected. The following variables were recorded (Figure 5.3):\n• Body weight of the infant in g (weight)\n• Body height of the infant in cm (height)\n• Head circumference in cm (headc)\n• Gender of the infant (gender: Female, Male)\n• Birth order in their family (parity: Singleton, One sibling, 2 or more siblings)\n• Education of the mother (education: tertiary, year10, year12)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear regression (1)</span>"
    ]
  },
  {
    "objectID": "linear1.html#research-question",
    "href": "linear1.html#research-question",
    "title": "5  Linear regression (1)",
    "section": "5.2 Research question",
    "text": "5.2 Research question\nLet’s say that we want to model the association between height and weight for the sample of 550 infants of 1 month age. In other words, we want to find the parameters of a mathematical equation, such as \\(y = \\beta_o + \\beta_1 \\cdot x\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear regression (1)</span>"
    ]
  },
  {
    "objectID": "linear1.html#hypothesis-testsing",
    "href": "linear1.html#hypothesis-testsing",
    "title": "5  Linear regression (1)",
    "section": "5.3 Hypothesis Testsing",
    "text": "5.3 Hypothesis Testsing\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\\(H_{0}:\\) the two variables are not linearly related. There is no effect between height and weight (\\(β_1 = 0\\)).\n\\(H_{1}:\\) the two variables are linearly related. There is an effect between height and weight (\\(β_1 \\neq 0\\)).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear regression (1)</span>"
    ]
  },
  {
    "objectID": "linear1.html#scatter-plot",
    "href": "linear1.html#scatter-plot",
    "title": "5  Linear regression (1)",
    "section": "5.4 Scatter plot",
    "text": "5.4 Scatter plot\nWe start our analysis by creating the scatter plot of the response variable weight and the explanatory variable height. The pattern of the plotted points typically reveals the nature and strength of the association between the two variables.\nOn the Jamovi top menu navigate to\n\n\n\n\n\nflowchart LR\n  A(Analyses) -.-&gt; B(Exploration) -.-&gt; C(Scatterplot)\n\n\n\n\n\n\nas shown below in Figure 5.4.\n\n\n\n\n\n\nFigure 5.4: In the menu at the top, choose Analyses &gt; Exploration  &gt; Scatterplot.\n\n\n\nIn the ANOVA dialog box, highlight height in the left panel and drug it to the the X-axis. Then highlight the weight drug it to the Y-axis (Figure 5.5). Additionally, check from the Regression Line “Linear”.\n\n\n\n\n\n\nFigure 5.5: The Scatterplot dialogue box options.\n\n\n\n\n\n\n\n\n\nFigure 5.6: The Scatter plot of height and weight.\n\n\n\nAs we can see in Figure 5.6, the points seem to be scattered around a line. The scatter plot also shows that, in general, infants with high height tend to have high weight (positive association).\nTo select the best fitting straight line of the data set, it is necessary to determine the estimated values \\(b_o\\) and \\(b_1\\) of parameters \\(\\beta_o\\) and \\(\\beta_1\\) in Equation 5.1. The regression equation of the model becomes:\n\\[\\widehat{y} = b_o  + b_1 \\cdot x \\tag{5.2}\\]\nWhy do we put a “hat” on top of the \\(y\\)? It’s a form of notation commonly used in regression to indicate that we have a predicted value, or the value of \\(y\\) on the regression line for a given \\(x\\) value.\n\n5.4.1 Linear regression\nThe process of fitting a linear regression model to the data involves finding a straight line that can be considered as the best representation of the overall association between age and lung capacity.\nTo choose a line, we need to explain what we mean by the “best representation” of the data. A “best-fitting” line refers to the line that minimizes the sum of squared residuals (RSS). Therefore, we refer to the resulting model as the least-squares linear regression model and to the corresponding line as the least-squares regression line.\n\n\n5.4.2 Fit a simple linear regression model\nOn the Jamovi top menu navigate to\n\n\n\n\n\nflowchart LR\n  A(Analyses) -.-&gt; B(Regression) -.-&gt; C(Linear Regression)\n\n\n\n\n\n\nas shown below (Figure 5.7).\n\n\n\n\n\n\nFigure 5.7: In the menu at the top, choose Analyses &gt; Regression  &gt; Linear Regression.\n\n\n\nThe Linear Regression dialogue box opens (Figure 5.8). From the left-hand pane drag the variable weight into the Dependent Variable field and the variable height into the Covariates field on the right-hand side, as shown below:\n\n\n\n\n\n\nFigure 5.8: The Linear Regression dialogue box options. Drag and drop the weight into the Dependent Variable field and the height into the Covariates field.\n\n\n\nNext, from the Assumption Checks section tick the box “Q-Q plot of residuals” (Figure 5.9).\n\n\n\n\n\n\nFigure 5.9: Assumption Checks choices.\n\n\n\n\n\n\n\n\n\nFigure 5.10: Assumption Checks choices.\n\n\n\nAdditionally, from the Model Coefficients section tick the box “Confidence interval” in Estimate (Figure 5.11):\n\n\n\n\n\n\nFigure 5.11: Check the Confidence interval box in the Model Coefficients section.\n\n\n\nThe output table with the model coefficients should look like the following (Figure 5.12):\n\n\n\n\n\n\nFigure 5.12: The model coefficients table.\n\n\n\n \nNow, let’s focus on interpreting the regression table in Figure 5.12. In the estimate column are the intercept \\(b_o\\) = -5411.95 and the slope \\(b_1\\) = 178.3 for height. Thus the equation of the regression line becomes:\n\\[\n\\begin{aligned}\n\\widehat{y} &= b_o + b_1 \\cdot x\\\\\n\\widehat{\\text{weight}} &= b_o + b_1 \\cdot\\text{height}\\\\\n\\widehat{\\text{weight}}&= -5411.953 + 178.296\\cdot\\text{height}\n\\end{aligned}\n\\]\n \nThe intercept \\(b_o\\)\nThe intercept \\(b_o\\) = -5411.95 is the average weight for those infants with height of 0. In graphical terms, it’s where the line intersects the \\(y\\) axis when \\(x\\) = 0 (Figure 5.13). Note, however, that while the intercept of the regression line has a mathematical interpretation, it has no physical interpretation here, since observing a weight of 0 is impossible.\n\n\n\n\n\n\n\n\nFigure 5.13: Data of infants’ body height-body weight with fitted line crossing the y-axis.\n\n\n\n\n\n \nThe slope \\(b_1\\)\nOf greater interest is the slope of height, \\(b_1\\)=\\(178.3\\), as it summarizes the association between the height and weight variables.\n\n\n\n\n\n\n\n\nFigure 5.14: Scatter plot of infants’ body height-body weight and graphically calculation of the slope.\n\n\n\n\n\nThe graphical calculation of the slope from two points of the fitted line is (Figure 5.14):\n\\[  \nb =\\frac{dy}{dx}=\\frac{5270-4560}{60-56}= \\frac{710}{4} \\approx 178\n\\] Note that, in this example, the coefficient has units g/cm.\nAdditionally, note that the sign is positive, suggesting a positive association between these two variables, meaning infants with higher height also tend to have higher weight. Recall from earlier that the correlation coefficient was \\(r = 0.71\\). They both have the same positive sign, but have a different value. Recall further that the correlation’s interpretation is the “strength of linear association”. The slope’s interpretation is a little different:\n\nFor every 1 cm increase in height, there is on average an associated increase of 178 g of weight.\n\nWe only state that there is an associated increase and not necessarily a causal increase. In other words, just because two variables are strongly associated, it doesn’t necessarily mean that one causes the other. This is summed up in the often quoted phrase, “correlation is not necessarily causation.”\nFurthermore, we say that this associated increase is on average 178 g of weight, because we might have two infants whose height differ by 1 cm, but their difference in weight won’t necessarily be exactly 178. What the slope of 178 is saying is that across all possible infants, the average difference in weight between two infants whose height differ by 1 cm is 178 g.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear regression (1)</span>"
    ]
  },
  {
    "objectID": "linear1.html#the-standard-error-se-of-the-regression-slope",
    "href": "linear1.html#the-standard-error-se-of-the-regression-slope",
    "title": "5  Linear regression (1)",
    "section": "5.5 The Standard error (SE) of the regression slope",
    "text": "5.5 The Standard error (SE) of the regression slope\nThe third column of the regression table in Figure 5.12 corresponds to the standard error of our estimates. We are interested in understanding the standard error of the slope (\\(SE_{b}\\)).\n\nSay we hypothetically collected 1000 samples of pairs of weight and height, computed the 1000 resulting values of the fitted slope \\(b\\), and visualized them in a histogram. This would be a visualization of the sampling distribution of \\(b\\). The standard deviation of the sampling distribution of \\(b\\) has a special name: the standard error of \\(b\\).\n\nThe coefficient for the independent variable ‘height’ is 178.31. The standard error is 7.49, which is a measure of the variability around this estimate for the regression slope.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear regression (1)</span>"
    ]
  },
  {
    "objectID": "linear1.html#test-statistic-and-confidence-intervals-for-the-slope",
    "href": "linear1.html#test-statistic-and-confidence-intervals-for-the-slope",
    "title": "5  Linear regression (1)",
    "section": "5.6 Test statistic and confidence intervals for the slope",
    "text": "5.6 Test statistic and confidence intervals for the slope\nThe 6th column of the regression table in Figure 5.12 corresponds to a t-statistic. The hypothesis testing for the slope is:\n\\[\n\\begin{aligned}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs } H_1&: \\beta_1 \\neq 0.\n\\end{aligned}\n\\]\nThe null hypothesis, \\(H_{0}\\), states that the coefficient of the independent variable (height) is equal to zero, and the alternative hypothesis, \\(H_{1}\\), states that the coefficient of the independent variable is not equal to zero.\nThe t-statistic for the slope is defined by the following equation:\n\\[\\ t = \\frac{\\ b_1}{\\text{SE}_{b_1}} \\tag{5.3}\\]\nIn our example:\n\\[\\ t = \\frac{\\ b_1}{\\text{SE}_{b_1}}=\\frac{\\ 178.31}{\\text{7.49}} = 23.81\\]\nIn practice, we use the p-value (as generated by Jamovi based on the value of the t-statistic Equation 5.3) to guide our decision:\n\nIf p − value &lt; 0.05, reject the null hypothesis, \\(H_{0}\\).\nIf p − value ≥ 0.05, do not reject the null hypothesis, \\(H_{0}\\).\n\nIn our example p &lt;0.001 \\(\\Rightarrow\\) reject \\(H_{0}\\).\nThe \\(95\\%\\) CI of the coefficient \\(b\\) for a significance level α = 0.05, \\(df=n-2\\) degrees of freedom and for a two-tailed t-test is given by:\n\\[ 95\\% \\ \\text{CI}_{b} = b \\pm t_{df; 0.05/2} \\cdot \\text{SE}_{b_1} \\tag{5.4}\\]\nIn our example:\n\\[ 95\\% \\ \\text{CI}_{b_1} = 178.31 \\pm 1.96 \\cdot \\text{7.49}= 178.31 \\pm 14.68 \\Rightarrow 95\\% \\text{CI}_{b_1}= \\ (163.6, 193)\\]  \n\n\n\n\n\n\nInterpretation of linear regression\n\n\n\nIn summary, we can say that the regression coefficient of the height (178) is significantly different from zero (p &lt; 0.001) and indicates that there’s on average an increase of 178 g (\\(95\\%\\)CI: 164 to 193) in weight for every 1 cm increase in height. Note that the \\(95\\%\\)CI does not include the hypothesized null value of zero for the slope.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear regression (1)</span>"
    ]
  },
  {
    "objectID": "linear1.html#observed-predicted-fitted-values-and-residuals",
    "href": "linear1.html#observed-predicted-fitted-values-and-residuals",
    "title": "5  Linear regression (1)",
    "section": "5.7 Observed, predicted (fitted) values and residuals",
    "text": "5.7 Observed, predicted (fitted) values and residuals\nWe define the following three concepts:\n\nObserved values \\(y\\), or the observed value of the dependent variable for a given \\(x\\) value\nPredicted (or fitted) values \\(\\widehat{y}\\), or the value on the regression line for a given \\(x\\) value\nResiduals \\(y - \\widehat{y}\\), or the error (ε) between the observed value and the predicted value for a given \\(x\\) value\n\n\n\n\n\n\n\nFigure 5.15: The equation of line.\n\n\n\nThe residuals are exactly the vertical distance between the observed data point and the associated point on the regression line (predicted value) (Figure 5.15). Positive residuals have associated y values above the fitted line and negative residuals have values below. We want the residuals to be small in magnitude, because large negative residuals are as bad as large positive residuals.\nFigure 5.16 shows these values:\n\n\n\n\n\n\n\n\nID\n\n\nweight\n\n\nheight\n\n\nweight_hat\n\n\nresidual\n\n\n\n\n\n\n1\n\n\n3950\n\n\n55.5\n\n\n4483.486\n\n\n-533.486\n\n\n\n\n2\n\n\n4630\n\n\n57.0\n\n\n4750.930\n\n\n-120.930\n\n\n\n\n3\n\n\n4750\n\n\n56.0\n\n\n4572.634\n\n\n177.366\n\n\n\n\n4\n\n\n3920\n\n\n56.0\n\n\n4572.634\n\n\n-652.634\n\n\n\n\n5\n\n\n4559\n\n\n55.0\n\n\n4394.338\n\n\n164.662\n\n\n\n\n6\n\n\n3639\n\n\n51.5\n\n\n3770.301\n\n\n-131.301\n\n\n\n\n7\n\n\n3550\n\n\n56.0\n\n\n4572.634\n\n\n-1022.634\n\n\n\n\n8\n\n\n4530\n\n\n57.0\n\n\n4750.930\n\n\n-220.930\n\n\n\n\n9\n\n\n4969\n\n\n58.5\n\n\n5018.375\n\n\n-49.375\n\n\n\n\n10\n\n\n3740\n\n\n52.0\n\n\n3859.449\n\n\n-119.449\n\n\n\n\n\n\nFigure 5.16: Regression points (first 10 out of 550 infants).\n\n\n\n\nObserve in the above table that weight_hat contains the predicted (fitted) values \\(\\widehat{y}\\) = \\(\\widehat{\\text{weight}}\\).\nThe residual column is simply \\(e_i = y - \\widehat{y} = weight - weight\\_hat\\).\nLet’s see, for example, the values for the first infant and have a visual representation:\n\nThe observed value \\(y\\) = 3950 is infant’s weight for \\(x\\) = 55.5.\nThe predicted value \\(\\widehat{y}\\) is the value 4483.939 on the regression line for \\(x\\) = 55.5. This value is computed using the intercept and slope in the previous regression in Figure 5.16: \\[\\widehat{y} = b_o + b_1 \\cdot x = -5411.953 + 178.296 \\cdot 55.5 = 4483.48\\]\nThe residual is computed by subtracting the predicted (fitted) value \\(\\widehat{y}\\) from the observed value \\(y\\). The residual can be thought of as a model’s error or “lack of fit” for a particular observation. In the case of this infant, it is \\(y - \\widehat{y}\\) = 3950 - 4483.4 = -533.4 .\n\nA “best-fitting” line refers to the line that minimizes the sum of squared residuals (RSS), also known as sum of squared estimate of errors (SSE) out of all possible lines we can draw through the points. The method of least squares is the most popular method used to calculate the coefficients of the regression line.\n\\[ min(RSS) =min\\sum_{i=1}^{n}(y_i - \\widehat{y}_i)^2  \\tag{5.5}\\]\nIn Figure 5.17, we have found the minimum value of RSS (it turns out to be 97723317) and have drawn a horizontal dashed green line. At the point where this minimum touches the graph, we have read down to the x axis to find the best value of the slope. This is the value 178.\n\n\n\n\n\n\n\n\nFigure 5.17: The sum of the squares of the residuals against the value of the coefficient of the slope which we are trying to estimate.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear regression (1)</span>"
    ]
  },
  {
    "objectID": "linear1.html#quality-of-a-linear-regression-fit",
    "href": "linear1.html#quality-of-a-linear-regression-fit",
    "title": "5  Linear regression (1)",
    "section": "5.8 Quality of a linear regression fit",
    "text": "5.8 Quality of a linear regression fit\nThe quality of a linear regression fit is typically assessed using two related quantities: residual standard error (RSE) and the coefficient of determination R\\(^2\\).\nResidual standard error (RSE)\nRSE represents the average distance that the observed values fall from the regression line. Conveniently, it tells us how wrong the regression model is on average using the units of the response variable. Smaller values are better because it indicates that the observations are closer to the fitted line. In our example:\n\\[\\ RSE = \\sqrt{\\frac{\\ RSS}{n-2}}= \\sqrt{\\frac{\\ 97723317}{550-2}}= 422.3 \\tag{5.6}\\]\n \nCoefficient of determination R\\(^2\\)\nThe quality of our simple linear model is presented in Figure 5.18:\n\n\n\n\n\n\nFigure 5.18: The coefficient of determination \\(R^2\\).\n\n\n\nThe R\\(^2\\) is the fraction of the total variation in \\(y\\) that is explained by the regression.\n\\[\\ R^2 = \\frac{\\ explained \\ \\ variation}{total \\ \\ variation} \\tag{5.7}\\]\nThe R\\(^2\\) value is called the coefficient of determination and indicates the percentage of the variance in the dependent variable that can be explained or accounted for by the independent variable. Hence, it is a measure of the ‘goodness of fit’ of the regression line to the data. It ranges between 0 and 1 (it won’t be negative). An R\\(^2\\) statistic that is close to 1 indicates that a large proportion of the variability in the response has been explained by the regression. A number near 0 indicates that the regression did not explain much of the variability in the response.\nIn our example takes the value 0.509. It indicates that about 50.9% of the variation in infant’s body weight can be explained by the variation of the infant’s body height. In simple linear regression \\(\\sqrt{0.509} = 0.713\\) which equals to the Pearson’s correlation coefficient, r.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear regression (1)</span>"
    ]
  },
  {
    "objectID": "linear1.html#simple-linear-regression-with-a-binary-explanatory-variable",
    "href": "linear1.html#simple-linear-regression-with-a-binary-explanatory-variable",
    "title": "5  Linear regression (1)",
    "section": "5.9 Simple linear regression with a binary explanatory variable",
    "text": "5.9 Simple linear regression with a binary explanatory variable\nUsing the same sample of 550 infants of 1 month age we want to examine how body weight is associated with the gender of the infant. Now we have an explanatory variable x that is binary (Male/Female), as opposed to the numerical explanatory variable model (height) that we used previously.\nIn the Linear Regression dialogue box opens (Figure 5.19) drag the variable gender into the Factors field.\n\n\n\n\n\n\nFigure 5.19: The Linear Regression dialogue box options. Drag and drop the weight into the Dependent Variable field and the height into the Factors field.\n\n\n\nA graphical comparison of the weight between the males and females is presented below using the JJStatsPlot:\n\n\n\n\n\n\nFigure 5.20: Violin plot by gender.\n\n\n\nHow can we handle this variable in a mathematical equation? Well, we will use a trick. All cases in which the respondent is Male will be coded as 1 and all other cases, in which the respondent is Female, will be coded as 0 (reference category). This allows us to enter in the gender values as numerical (note that these numbers are just indicators).\n\\[\n\\text{gender} =\n\\begin{cases}\n1 & \\text{if infant is Male} \\\\\n0 & \\text{otherwise (ref.)}\n\\end{cases}\n\\]\nThe equation of the regression line will have the following form:\n\\[\n\\begin{aligned}\n\\widehat{y} &= b_o + b_1 \\cdot x\\\\\n\\widehat{\\text{weight}} &= b_o + b_1 \\cdot\\text{gender}\n\\end{aligned}\n\\]\nAdditionally, from the Model Coefficients section tick the box “Confidence interval” in Estimate (Figure 5.11):\n\n\n\nCheck the Confidence interval box in the Model Coefficients section.\n\n\nThe output table with the model coefficients should look like the following (Figure 5.21):\n\n\n\n\n\n\nFigure 5.21: The model coefficients table.\n\n\n\nThe equation of the model becomes:\n\\[\n\\begin{aligned}\n\\widehat{\\text{weight}} &= b_o + b_1 \\cdot\\text{gender}\\\\\n\\widehat{\\text{weight}} &= 4140 + 452 \\cdot\\text{gender}\n\\end{aligned}\n\\]\n\nThe “intercept” represents the average weight of a female infant, which is 4140 g and serves as the reference category.\nThe “gender” term denotes the average weight difference between male and female infants, which is 452 g.\n\nTherefore, the mean weight of a male infant is (4140 + 452) 4592 g which is significantly higher (on average) about 452 g relative to a female infant (p&lt;0.001). The 95% confidence interval for this estimation (the difference in means) is 358 to 545 g.\nIt is important to note that the above analysis is equivalent to run a two-sample t-test.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear regression (1)</span>"
    ]
  },
  {
    "objectID": "linear1.html#simple-linear-regression-with-a-categorical-explanatory-variable-2-categories",
    "href": "linear1.html#simple-linear-regression-with-a-categorical-explanatory-variable-2-categories",
    "title": "5  Linear regression (1)",
    "section": "5.10 Simple linear regression with a categorical explanatory variable (> 2 categories)",
    "text": "5.10 Simple linear regression with a categorical explanatory variable (&gt; 2 categories)\nSuppose that infants are categorized into three categories based on parity: singletons, having one sibling, or having 2 or more siblings. We choose as the reference category the singleton infants.\nIn the Linear Regression dialogue box opens (Figure 5.19) drag the variable gender into the Factors field.\n\n\n\n\n\n\nFigure 5.22: The Linear Regression dialogue box options. Drag and drop the weight into the Dependent Variable field and the parity into the Factors field.\n\n\n\nA graphical comparison of the weight between the males and females is presented below using the JJStatsPlot:\n\n\n\n\n\n\nFigure 5.23: Violin plot by gender.\n\n\n\nWe will use the previous trick and we will create 2 dummy variables to assign numerical values to the levels of parity. So each dummy variable will represent one category of the explanatory variable and will be coded with 1 if the case falls in that category and with 0 if not.\n\\[\n\\text{parity1} =\n\\begin{cases}\n1 & \\text{if infant has one sibling} \\\\\n0 & \\text{otherwise (ref.)}\n\\end{cases}\n\\]\n\\[\n\\text{parity2} =\n\\begin{cases}\n1 & \\text{if infant has 2 or more siblings} \\\\\n0 & \\text{otherwise (ref.)}\n\\end{cases}\n\\]\nThe equation of the regression line will have the following form:\n\\[\n\\begin{aligned}\n\\widehat{y} &= b_o + b_1 \\cdot x\\\\\n\\widehat{\\text{weight}} &= b_o + b_1 \\cdot\\text{parity1} + b_2 \\cdot\\text{parity2}\n\\end{aligned}\n\\]\nTherefore, we are including all the categories to the linear regression model except the one which is going to be used as the reference category (here is the Singleton). Actually, we create a multiple regression model which we will examine later analytically.\nWe also select from the Reference Level the “Singleton” category and from the Model Coefficients section tick the box “Confidence interval” in Estimate (Figure 5.24):\n\n\n\n\n\n\nFigure 5.24: Select the reference level and check the Confidence interval box in the Model Coefficients section.\n\n\n\nThe output table with the model coefficients should look like the following (Figure 5.25):\n\n\n\n\n\n\nFigure 5.25: The model coefficients table.\n\n\n\nThe equation of the model becomes:\n\\[\n\\begin{aligned}\n\\widehat{\\text{weight}} &= b_o + b_1 \\cdot\\text{parity1} + b_2 \\cdot\\text{parity2}\\\\\n\\widehat{\\text{weight}} &= 4259 + 130 \\cdot\\text{parity1} + 192 \\cdot\\text{parity2}\n\\end{aligned}\n\\]\n\nThe intercept corresponds to the mean weight 4259 g for a singleton infant which is the reference category.\nThe mean weight of an infant with one sibling is 4389 g which is significantly higher (on average) about 130 g relative to a singleton infant (p=0.037). The 95% confidence interval for this estimation (the difference in means) is 8 to 252 g.\nThe mean weight of an infant with 2 or more siblings is 4451 g which is significantly higher (on average) about 192 g relative to a singleton infant (p=0.002). The 95% confidence interval for this estimation (the difference in means) is 68 to 316 g.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear regression (1)</span>"
    ]
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "6  Presentations",
    "section": "",
    "text": "Lecture 1: Introduction\n\n\n\n\npsy-introduction\n\n\n     \n\nLecture 2: Between-subjects: One-way ANOVA\n\n\n\n\npsy-anova1\n\n\n     \n\nLecture 3: Within-subjects: Repeated ANOVA\n\n\n\n\npsy-anova2\n\n\n     \n\nLecture 4: Within-subjects: Two-way ANOVA\n\n\n\n\npsy-anova3",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Presentations</span>"
    ]
  }
]