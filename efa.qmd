# Exploratory Factor Analysis {#sec-efa}

```{r}
#| include: false

library(fontawesome)


library(here)
library(tidyverse)


library(readxl)
dat <- read_excel(here("data", "dfGRMS.xlsx"))
```

When we have finished this chapter, we should be able to:

::: {.callout-caution icon="false"}
## Learning objectives

-   Understand when and why to use Exploratory Factor Analysis (EFA), and recognize appropriate data conditions
-   Perform EFA through a step-by-step process
-   Interpret factor loadings and the proportion of explained variance
-   Extract and interpret factors for further analysis
-   Assess internal consistency (reliability).
:::

## Introduction

As we discussed in the previous chapter, Principal Component Analysis (PCA) is primarily used to reduce the dimensionality of the dataset while retaining as much of its total variance as possible. In this chapter, Exploratory Factor Analysis (EFA) focuses on identifying underlying **latent factors** that explain the correlations among observed (also known as manifest) variables, with the aim of uncovering the data's underlying structure.

::: content-box-blue
**Exploratory Factor Analysis**

Exploratory Factor Analysis (EFA) can be used to explore the dimensionality of a measurement instrument by identifying the smallest number of interpretable factors that explain the **common variance** among variables.
:::

**EXAMPLE DATA**

We will use the same data from the study by Lewis and Neville on the Gendered Racial Microaggressions Scale for Black Women.

```{r}
#| echo: false
#| label: fig-dfGRMS0
#| fig-cap: Table with raw data.

DT::datatable(dat, extensions = c('Scroller', 'Buttons'), rownames = FALSE,
              options = list(deferRender = F, dom = 'Bt', scrollY = 452, scroller = TRUE, scrollX = T, pageLength = 10, buttons = 
      list(list(
        extend = 'csv',
        filename = 'dfGRMS'
      ))))

```

Lewis and Neville [@lewis2015] provided support for both a total scale score, consisting of 25 items, and four subscales. Below, we outline the four subscales, including the number of items in each, their abbreviations, and a sample item representing each factor:

-   Assumptions of Beauty and Sexual Objectification (10 items)
    -   Unattractive because of size of butt (Obj1)
    -   Negative comments about size of facial features (Obj2)
    -   Imitated the way they think Black women speak (Obj3)
    -   Someone made me feel unattractive (Obj4)
    -   Negative comment about skin tone (Obj5)
    -   Someone assumed I speak a certain way (Obj6)
    -   Objectified me based on physical features(Obj7)
    -   Someone assumed I have a certain body type (Obj8)
    -   Made a sexually inappropriate comment (Obj9)
    -   Negative comments about my hair when natural (Obj10)
-   Silenced and Marginalized (7 items)
    -   I have felt unheard (Marg1)
    -   My comments have been ignored (Marg2)
    -   Someone challenged my authority (Marg3)
    -   I have been disrespected in workplace (Marg4)
    -   Someone has tried to “put me in my place” (Marg5)
    -   Felt excluded from networking opportunities (Marg6)
    -   Assumed I did not have much to contribute to the conversation (Marg7)
-   Strong Black Woman Stereotype (5 items)
    -   Someone assumed I was sassy and straightforward (Str1)
    -   I have been told that I am too independent (Str2)
    -   Someone made me feel exotic as a Black woman (Str3)
    -   I have been told that I am too assertive (Str4)
    -   Assumed to be a strong Black woman (Str5)
-   Angry Black Woman Stereotype (3 items)
    -   Someone has told me to calm down (Ang1)
    -   Perceived to be “angry Black woman" (Ang2)
    -   Someone accused me of being angry when speaking calm (Ang3)

## Steps in the process of EFA

-   **Prepare the Data:** Standardize the data. In practice, Likert scale data are typically not standardized, as the scale is generally considered consistent across items. (b) Address outliers in the data. Note that outliers in Likert data are often extreme values, such as respondents who consistently select the “0 = not at all stressful” or “5 = extremely stressful” option. (c) Additionally, it is important to reverse-score negatively worded items to ensure that all items are scaled in the same direction (though this step is not applicable in our example).

-   **Evaluate Assumptions:** Assess the suitability of the data for EFA using diagnostic tests such as the Kaiser-Meyer-Olkin (KMO) measure of sampling adequacy and Bartlett’s test of sphericity. Additionally, examine the correlation matrix to ensure that variables are sufficiently correlated for factor analysis but without multicollinearity (extremely high correlations) or singularity (perfect correlations of 1 or -1). These tests determine whether the correlation matrix is appropriate for factor extraction.

-   **Choose the Extraction Method:** Select an extraction method for factors (e.g., Principal Axis Factoring, Maximum Likelihood, or Maximum Residuals). Principal Axis Factoring is commonly used when you assume that underlying latent factors are driving correlations.

-   **Determine the Number of Factors:** Identify how many factors to extract, guided by criteria such as eigenvalues greater than one, scree plot inspection, or parallel analysis. These components often correspond to underlying subscales.

-   **Factor Rotation:** Apply a rotation method to simplify the factor structure. Orthogonal rotation (e.g., Varimax) assumes factors are uncorrelated. Oblique rotation (e.g., Oblimin, Promax) assumes factors are correlated.

-   **Interpret the Factors:** Examine the factor loadings to interpret each factor. Look for variables that load highly on each factor and assign meaningful labels to the factors.

-   **Assess Factor Reliability:** Use Cronbach's Alpha to assess the internal consistency or reliability of the factors. A value above 0.7 indicates good reliability.

-   **Refine the Model:** Based on the results, you may choose to drop items with low factor loadings or revise the number of factors. Ensure that any issues of singularity are addressed by removing highly correlated variables.

-   **Finalize the Factor Solution:** Once the factors are interpretable and reliable, finalize the model and prepare the results for reporting.

## Example of GRMS Stress Appraisal

On the Jamovi top menu navigate to

```{mermaid}
flowchart LR
  A(Analyses) -.-> B(Factor) -.-> C(Exploratory Factor Analysis)
```

as shown below (@fig-fa1).

![Select Exploratory Factor Analysis from Factor.](images/fa1.png){#fig-fa1 width="70%"}

The *Exploratory Factor Analysis box* opens. From the left-hand pane drag all the variables into the *Variables* field on the right-hand side, as shown below (@fig-fa2):

![Exploratory Factor Analysis box options. Drag and drop the variables in the fields on the right-hand side.](images/fa2.png){#fig-fa2 width="65%"}

All variables in the dataset are on the same scale (Likert scale).

### Assumptions

The assumption checks for EFA are similar to those in PCA (Bartlett’s test of sphericity, Kaiser-Meyer-Olkin (MKO) index of Sampling Adequacy; see @sec-pca). However, in Exploratory Factor Analysis, we are also particularly concerned with issues of multicollinearity and singularity in the data. One way to assess this is by examining the determinant of the correlation matrix. If the determinant is smaller than 0.00001, this may indicate that some variables are too highly correlated (multicollinearity), or some variables are perfectly correlated (singularity), which can distort factor extraction.

In our example, the determinant of the correlation matrix can be calculated using a simple line of code in the **Rj Editor+** of Jamovi:

![Calculation of determinant of correlation matrix in RJ Editor.](images/Rj1.png){#fig-Rj1 width="55%"}

```{r, echo=FALSE}
round(det(cor(dat)), 4) 
```

With a value of 0.0075, the determinant is comfortably above the 0.00001 threshold, indicating that our data is appropriate for factor analysis.

If the determinant were below the threshold, we would need to identify and address problematic variables—those that are either overly correlated or not correlated sufficiently with others—and then re-run the diagnostic checks.

### Specify the Number of Factors

When researchers lack a clear theoretical framework to guide their analysis, they often adopt an iterative approach, exploring multiple solutions by extracting varying numbers of factors. For example, Lewis and Neville (2015) examined models with two, three, four, and five factors. In this example, we will present a four-factor solution as the selected model:

-   **Factor 1:** Assumptions of Beauty and Sexual Objectification
-   **Factor 2:** Silenced and Marginalized
-   **Factor 3:** Strong Woman Stereotype
-   **Factor 4:** Angry Woman Stereotype

Therefore, we select "Fixed number" and type 4 in the number of factors:

![Select the number of factor for a four-factor model.](images/efa4004.png){#fig-efa4004 width="30%"}

We can also obtain the scree plot from the Additional Output:

![Select Scree plot from the Additional Output.](images/efa4005.png){#fig-efa4005 width="25%"}

![Scree plot.](images/efa4006.png){#fig-efa4006 width="65%"}


### Extraction of factors and rotation

Lewis and Neville (2015) used parallel analysis as their extraction method. In this example, we demonstrate the use of **Principal Axis Factoring** (PAF) for extraction and Oblimin (the default option) as the rotation method.


![Select the Principal Axis for extraction and Oblimin method of rotation.](images/fa3.png){#fig-fa3 width="35%"}

Additionally, we hide loading below 0.23 (the default is 0.3):

![Hide loadings below 0.23.](images/fa9.png){#fig-fa9 width="28%"}

@fig-fa4 shows the loadings of the factors after Oblimin rotation.

![Factor Loadings using Pricipal axis factoring in combination with Oblimin rotation.](images/fa4.png){#fig-fa4 width="55%"}

In the rotated factor matrix, a few items exhibit cross-loadings, meaning they load significantly on more than one factor:

-   Obj6 loads on both Factor 1 (Objectification) and Factor 4 (Angry), with loadings of 0.43 and 0.25, respectively. While the item shows some association with both factors, its stronger loading on Factor 1 suggests it aligns more closely with the Objectification construct.

-   Marg6 also displays cross-loadings, with loadings of 0.31 on Factor 1 (Objectification) and 0.25 on Factor 2 (Marginalized). Though both values are relatively low, the item may still require closer evaluation to determine its conceptual alignment.

-   Ang1 loads on both Factor 3 (Strong) and Factor 4 (Angry), with loadings of 0.33 and 0.24, respectively. Although neither loading is particularly high, this item should be reviewed in the context of theoretical expectations.

**NOTE:** In oblique rotation, a distinction is made between the pattern matrix and the structure matrix. The **pattern matrix** (@fig-fa4) displays the standardized regression coefficients (i.e., factor loadings) of each factor on each observed variable, reflecting the direct effects while controlling for the influence of other factors. In contrast, the **structure matrix** presents the simple correlations between each observed variable and each factor, reflecting both direct and indirect effects (due to factor intercorrelations) without controlling for the overlap among factors (this matrix is not included in the Jamovi output).

### Explained variance and model fit

Under the **Additional Output** section, select *Factor summary*, *Factor correlations*, and *Model fit measures*:

![Select Factor summary, correlations, and model fit measures.](images/fa7.png){#fig-fa7 width="25%"}

 

**Explained variance**

@fig-fa5 presents the percentage of common variance in the variable set that is captured by the derived factors after Oblimin rotation.

![Common variance explained.](images/fa5.png){#fig-fa5 width="50%"}

Factor 1 explains 11.2% of the shared variance, Factor 2 explains an additional 8.1%, Factor 3 explains another 5.5%, and Factor 4 explains an additional 3.6%, resulting in a cumulative common variance of 28.4% explained by the first four factors.

Although explaining about 30% of the common variance may seem modest, it is not uncommon in social science research, particularly when dealing with psychological or behavioral constructs. These constructs are often complex and difficult to measure precisely, which can lead to lower levels of explained variance.

The Inter-Factor Correlation Matrix reveals that the four factors are positively correlated with each other, with correlations ranging from 0.17 to 0.46 (@fig-fa16).

![Inter-Factor Correlation Matrix.](images/fa16.png){#fig-fa16 width="45%"}

 

**Model fit**

The chi-square test can be used to assess whether the model fits the data. The null hypothesis for the chi-square test is that the model fits the data perfectly. Therefore, a non-significant chi-square value (e.g., p \> 0.05) indicates that the model fits the data reasonably well. However, the chi-square test is sensitive to sample size and non-normal variable distributions (@fig-fa21).

In addition to the chi-square test, the root mean square error of approximation (RMSEA) serves as an absolute fit index that takes model complexity into account by incorporating a penalty for lack of parsimony. RMSEA values less than or equal to 0.08 are typically interpreted as good model fit.

![Measures of model fit.](images/fa21.png){#fig-fa21 width="65%"}

In our model: $\chi^2(206)= 189.19$, p = 0.794, which is non-significant and suggests that the model fits the data well. Moreover, the RMSEA value is 0.00, indicating a close fit between the model and the observed data.

These results suggest that the model structure is appropriate, even if the strength of the factors is modest.

### Path diagram

The path diagram for the four-factor, 25 items EFA, showing only the dominant loading per item (for clarity) follows:

```{r, echo=FALSE, warning=FALSE, message=FALSE}

FA1 <- psych::fa(dat, nfactors = 4, fm = "pa", rotate = "oblimin")

colnames(FA1$loadings) <- c("F1", "F2", "F3", "F4")

psych::fa.diagram(FA1, cut = 0, digits = 2, l.cex=0.8,
                  marg=c(0.0, 0.5, 0.5, 0.1), sort = F)
```

It is important to note that the arrows point from the factor (oval) to the observed variables (items in squares) illustrating that the factor “explains” the item’s score. This represents a **reflective model**, in which the latent construct exists independently of the measures. Additionally, note that the path diagram includes links with numerical values between latent factors, representing the correlation coefficients among them.

### Factor scores

Factor scores can be estimated for each case (row) on each latent factor (unobserved variables). These scores can be used to investigate the associations between the factors and other variables or to represent the underlying factors for subsequent analysis. For example, factor scores are often used in regression models (in place of means or sums) when predictors are highly correlated, helping to address multicollinearity.

There are several estimation methods for obtaining factor scores in EFA using Jamovi, with each method having different assumptions and advantages.

When using Oblimin rotation (which allows factors to be correlated), the best methods for estimating factor scores are **ten Berge’s method** and **Bartlett’s method**. Ten Berge’s method is generally considered the most accurate because it specifically adjusts for factor correlations using generalized least squares. Bartlett’s method is also a strong choice, producing consistent scores while accounting for factor correlations. In contrast, Anderson-Rubin’s method should be avoided because it forces factor scores to be uncorrelated, which conflicts with the purpose of Oblimin. Thurstone’s method can be used but is less optimal, and Harman’s method is not recommended because it does not properly adjust for correlated factors.

In our example, we select the Ten Berge’s method (@fig-efa4002).

![Select Factor scores and ten Berge estimation method.](images/efa4002.png){#fig-efa4002 width="60%"}

 

![Factor scores usin ten Berge estimation method.](images/efa4003.png){#fig-efa4003 width="60%"}

## Reliability analysis

An important step in establishing the integrity of factors derived from a factor analysis (which will ultimately be presented as subscales of the scale) is to assess their internal consistency as an initial indicator of reliability.

On the Jamovi top menu navigate to

```{mermaid}
flowchart LR
  A(Analyses) -.-> B(Factor) -.-> C(Reliability Analysis)
```

as shown below (@fig-fa8).

![Select Reliability Analysis from Factor.](images/fa8.png){#fig-fa8 width="60%"}

 

We have four derived factors, so we will run Cronbach’s reliability analyis four times-once for each subset of observed variables (items) grouped by the factor analysis.

From the Scale Statistics options, select Cronbach’s $\alpha$ :

![Select Cronbach’s alpha from Scale Statistics.](images/fa10.png){#fig-fa10 width="60%"}

(Note that there is also a menu option for “Reverse Scaled Items” for items that need to have their scores reversed).

 

![Cronbach’s alpha for factor 1.](images/fa17.png){#fig-fa17 width="25%"}

 

![Cronbach’s alpha for factor 2.](images/fa18.png){#fig-fa18 width="25%"}

 

![Cronbach’s alpha for factor 3.](images/fa19.png){#fig-fa19 width="25%"}

 

![Cronbach’s alpha for factor 4.](images/fa20.png){#fig-fa20 width="25%"}

 

Next, based on theoretical guidance, we suppose that Marg6 is included in the second factor (Marginalized) and Ang1 in the fourth factor (Angry). We re-calculate the Cronbach’s alpha.

![Cronbach’s alpha for factor 1.](images/fa11.png){#fig-fa11 width="25%"}

 

![Cronbach’s alpha for factor 2.](images/fa12.png){#fig-fa12 width="25%"}

 

![Cronbach’s alpha for factor 3.](images/fa13.png){#fig-fa13 width="25%"}

 

![Cronbach’s alpha for factor 4.](images/fa14.png){#fig-fa14 width="25%"}

 

The overall Cronbach’s $\alpha$ is:

![Overall Cronbach’s alpha.](images/fa15.png){#fig-fa15 width="25%"}
